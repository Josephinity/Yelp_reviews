{
 "metadata": {
  "name": "",
  "signature": "sha256:fda5527ba3fdbe55348ed3f4e7f747e6d1e6b5cf6fcee73a7ca276500744681d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%cd~/GitHub/Yelp_reviews\n",
      "execfile('read_reviews_txt.py')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/xiaobaby/GitHub/Yelp_reviews\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reviews=read_reviews_txt('reviews_0_50.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "21675"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#plot distribution for pandas data\n",
      "pd.options.display.mpl_style = 'default'\n",
      "reviews.review_rating.astype(float).hist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "<matplotlib.axes._subplots.AxesSubplot at 0x10efb4a10>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEBCAYAAACe6Rn8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b7f1ed0>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#review tokenizer(& punctuation stripper)\n",
      "punctuations = set(['[','!','#','\"','%','&','\\\\','(',')','*'\n",
      "                    ,'+',',','-','.','/',':',';','<','=','>','?','@','[',']','^','_','`','{','|','}','~',']'])\n",
      "strip_punctuations = lambda x: ''.join([i if i not in punctuations else ' ' for i in x ])\n",
      "reviews['words']=[strip_punctuations(r.lower()).split() for r in reviews.review]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#apply naive bayes classifier\n",
      "import collections, itertools\n",
      "import nltk.classify.util, nltk.metrics\n",
      "from nltk.classify import NaiveBayesClassifier\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.collocations import *\n",
      "from nltk.metrics import BigramAssocMeasures,scores\n",
      "from nltk.probability import FreqDist, ConditionalFreqDist\n",
      "\n",
      "\n",
      "#create bag of words\n",
      "def all_words(data):\n",
      "    dic={}\n",
      "    for label in data.keys():\n",
      "        tokens = []\n",
      "        for words in data[label]:\n",
      "            tokens += words\n",
      "        dic[label] = tokens\n",
      "    return dic\n",
      "\n",
      "\n",
      "#initial run, include all words in analysis\n",
      "def word_feats(words):\n",
      "    return dict([(word.lower(), True) for word in words])    \n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#identify significant features                      ----nBestWords N, minFreq\n",
      "def get_nBestFeatures(labeled_words, N = 50):\n",
      "    word_fd = FreqDist()\n",
      "    label_word_fd = ConditionalFreqDist()\n",
      "    \n",
      "    #word count\n",
      "    word_count = {'total':0}\n",
      "    \n",
      "    for label in labeled_words.keys():\n",
      "        for word in labeled_words[label]:\n",
      "            word_fd[word.lower()]+=1\n",
      "            label_word_fd[label][word.lower()]+=1\n",
      "        word_count[label] = label_word_fd[label].N()\n",
      "        word_count['total'] += word_count[label]\n",
      "    \n",
      "    word_scores = {}\n",
      "    \n",
      "    for word, freq in word_fd.iteritems():\n",
      "        word_scores[word] = 0\n",
      "        for label in labeled_words.keys():\n",
      "            word_scores[word] += BigramAssocMeasures.chi_sq(label_word_fd[label][word],\n",
      "                (freq, word_count[label]), word_count['total'])            \n",
      "            \n",
      "    best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:N]\n",
      "    bestwords = set([w for w, s in best if word_fd[w]>=minFreq])\n",
      "    return bestwords\n",
      "\n",
      "#second run, include only nBestWords    \n",
      "def best_word_feats(words,bestwords):\n",
      "    return dict([(word, True) for word in words if word in bestwords])\n",
      "\n",
      "def informative_word_feats(words,informativewords):\n",
      "    return dict([(word, True) for word in words if word in informativewords])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#identify n-grams       ---minFreq, nBestNGrams , n best ngram features, minNgramFreq\n",
      "def find_ngrams(data):\n",
      "    volcabulary = []\n",
      "    for sublist in all_words(data).values():\n",
      "        volcabulary += sublist\n",
      "        \n",
      "    bigram_finder = BigramCollocationFinder.from_words(volcabulary)\n",
      "    bigram_finder.apply_freq_filter(minNgramFreq)\n",
      "    bigrams = bigram_finder.nbest(BigramAssocMeasures.pmi, 200)\n",
      "    trigram_finder = TrigramCollocationFinder.from_words(volcabulary)\n",
      "    trigram_finder.apply_freq_filter(minNgramFreq)\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    trigrams = trigram_finder.nbest(trigram_measures.pmi, 200)\n",
      "    return bigrams+trigrams\n",
      " \n",
      "#final run, include n-grams     \n",
      "def best_ngram_word_feats(words, ngrams, bestwords, score_fn=BigramAssocMeasures.chi_sq):\n",
      "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
      "    bigrams_ = bigram_finder.nbest(BigramAssocMeasures.pmi, nBestNGrams)\n",
      "    trigram_finder = TrigramCollocationFinder.from_words(words)\n",
      "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
      "    trigrams_ = trigram_finder.nbest(trigram_measures.pmi, nBestNGrams)\n",
      "    d = dict([(ngram, True) for ngram in bigrams_+trigrams_ if ngram in ngrams])\n",
      "    d.update(best_word_feats(words,bestwords))\n",
      "    return d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for getting most informative word feats \n",
      "def best_word_classifier(data):\n",
      "    bestwords = get_nBestFeatures(all_words(data),N=nBestFeatures)\n",
      "    features = {label:[(best_word_feats(words,bestwords),label) for words in data[label]] for label in data.keys()}\n",
      "\n",
      "    #features={label:[(word_feats(words),label) for words in data[label]] for label in data.keys()}\n",
      "    trainfeats=[]\n",
      "    for label in features.keys():\n",
      "        trainfeats += features[label][:len(features[label])*3/4]\n",
      "\n",
      "\n",
      "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
      "    return classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for getting most informative ngram feats \n",
      "def best_ngram_classifier(data):\n",
      "    ngrams = find_ngrams(data)\n",
      "    bestwords = get_nBestFeatures(all_words(data),N=nBestFeatures)\n",
      "    features = {label:[(best_ngram_word_feats(words,ngrams,bestwords),label) for words in data[label]] for label in data.keys()}\n",
      "    trainfeats=[]\n",
      "    for label in features.keys():\n",
      "        trainfeats += features[label][:len(features[label])*3/4]\n",
      "\n",
      "\n",
      "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
      "    return classifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evaluate_classifier(data,featx):\n",
      "    \n",
      "    bestwords = get_nBestFeatures(all_words(data),N=nBestFeatures)\n",
      "    if best_word_feats==featx:\n",
      "        features = {label:[(featx(words,bestwords),label) for words in data[label]] for label in data.keys()}\n",
      "    elif informative_word_feats==featx:\n",
      "        mostInformWords = [x[0] for x in best_word_classifier(data).most_informative_features(300)]\n",
      "        features = {label:[(featx(words,mostInformWords),label) for words in data[label]] for label in data.keys()}\n",
      "    elif best_ngram_word_feats==featx:\n",
      "        ngrams = find_ngrams(data)\n",
      "        features = {label:[(featx(words,ngrams,bestwords),label) for words in data[label]] for label in data.keys()}\n",
      "    else:\n",
      "        features={label:[(featx(words),label) for words in data[label]] for label in data.keys()}\n",
      "    \n",
      "    for label in features.keys():\n",
      "        print (label + 'sample = %i' %len(features[label]))\n",
      "        \n",
      "    trainfeats = []\n",
      "    testfeats = []\n",
      "    for label in features.keys():\n",
      "        trainfeats += features[label][:len(features[label])*3/4] #decide size of train/test cutoff here\n",
      "        testfeats += features[label][len(features[label])*3/4:]\n",
      "        \n",
      "    ###print 'training size:  %i, testing size: %i' %(len(trainfeats),len(testfeats))\n",
      " \n",
      "    classifier = NaiveBayesClassifier.train(trainfeats)\n",
      "    refsets = collections.defaultdict(set)  \n",
      "    testsets = collections.defaultdict(set)   \n",
      " \n",
      "    for i, (feats, label) in enumerate(testfeats):\n",
      "            refsets[label].add(i)\n",
      "            observed = classifier.classify(feats)\n",
      "            testsets[observed].add(i)\n",
      " \n",
      "    print 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats)\n",
      "    #return nltk.classify.util.accuracy(classifier, testfeats)\n",
      "    if(len(features)==2):\n",
      "        print 'pos precision:', scores.precision(refsets['pos'], testsets['pos'])\n",
      "        print 'pos recall:', scores.recall(refsets['pos'], testsets['pos'])\n",
      "        print 'neg precision:', scores.precision(refsets['neg'], testsets['neg'])\n",
      "        print 'neg recall:', scores.recall(refsets['neg'], testsets['neg'])\n",
      "    ###classifier.show_most_informative_features(n=15)\n",
      "    \n",
      "    print minNgramFreq\n",
      "    return nltk.classify.util.accuracy(classifier, testfeats)\n",
      "    #return (nltk.classify.util.accuracy(classifier, testfeats),scores.recall(refsets['pos'], testsets['pos']))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "high = reviews.words[reviews.review_rating.astype(float)>=3.5]\n",
      "low = reviews.words[reviews.review_rating.astype(float)<=3.5]\n",
      "data={\n",
      "    'pos':high,\n",
      "    'neg':low\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#n is top n most informative features\n",
      "def show_most_informative_features(classifier, word_fd, n=10):\n",
      "    # Determine the most relevant features, and display them.\n",
      "    cpdist = classifier._feature_probdist\n",
      "    print('Most Informative Features')\n",
      "    mif=[['word','label','probability']]\n",
      "    for (fname, fval) in classifier.most_informative_features(n):\n",
      "        if word_fd[fname]>=minFreq or len(fname)>1:\n",
      "            def labelprob(l):\n",
      "                return cpdist[l, fname].prob(fval)\n",
      "\n",
      "            labels = sorted([l for l in classifier._labels\n",
      "                             if fval in cpdist[l, fname].samples()],\n",
      "                            key=labelprob)\n",
      "            if len(labels) == 1:\n",
      "                continue\n",
      "            l0 = labels[0]\n",
      "            l1 = labels[-1]\n",
      "            if cpdist[l0, fname].prob(fval) == 0:\n",
      "                ratio = 'INF'\n",
      "            else:\n",
      "                ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) /\n",
      "                                   cpdist[l0, fname].prob(fval))\n",
      "    #         print(('%24s = %-14r %6s : %-6s = %s : 1.0' %\n",
      "    #                (fname, fval, (\"%s\" % l1)[:6], (\"%s\" % l0)[:6], ratio)))\n",
      "            mif+=[[fname,labels[-1],ratio]]\n",
      "    return mif"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for getting most informative word feats        \n",
      "###classifier = best_word_classifier(data)\n",
      "clf =best_ngram_classifier(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###mif = show_most_informative_features(clf, word_fd, n=100)\n",
      "mif = show_most_informative_features(clf, word_fd,50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n"
       ]
      }
     ],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get most informative features into dataframe\n",
      "def to_string(tup):\n",
      "    word=''\n",
      "    for x in tup:\n",
      "        word+=x+' '\n",
      "    return word\n",
      "mifd = {\n",
      "    mif[0][0]:[to_string(x[0]) if type(x[0]) is tuple else x[0] for x in mif[1:]],\n",
      "    mif[0][1]:[x[1] for x in mif[1:]],\n",
      "    mif[0][2]:[x[2] for x in mif[1:]]\n",
      "    }\n",
      "mifd=pd.DataFrame(mifd)\n",
      "mifd=mifd[['word','label','probability']][:30]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#to get term frequency\n",
      "labeled_words = all_words(data)\n",
      "word_fd = FreqDist()\n",
      "label_word_fd = ConditionalFreqDist()\n",
      "\n",
      "#word count\n",
      "word_count = {'total':0}\n",
      "\n",
      "for label in labeled_words.keys():\n",
      "    for word in labeled_words[label]:\n",
      "        word_fd[word.lower()]+=1\n",
      "        label_word_fd[label][word.lower()]+=1\n",
      "    word_count[label] = label_word_fd[label].N()\n",
      "    word_count['total'] += word_count[label]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#to get word score(normalization) with BigramAssoMeasures.chi_sq\n",
      "word_scores = {}\n",
      "\n",
      "for word, freq in word_fd.iteritems():\n",
      "    word_scores[word] = 0\n",
      "    for label in labeled_words.keys():\n",
      "        word_scores[word] += BigramAssocMeasures.chi_sq(label_word_fd[label][word],\n",
      "            (freq, word_count[label]), word_count['total'])            \n",
      "\n",
      "best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:50]\n",
      "bestwords = set([w for w, s in best if word_fd[w]>=minFreq])\n",
      "#words with most scores \n",
      "sorted(word_scores, key=word_scores.get, reverse=True)[:5]\n",
      "word_scores['best']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 135,
       "text": [
        "1154.3843291699661"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#get most frequent terms (before normalization)\n",
      "nn=1\n",
      "print label_word_fd['pos'].most_common(nn)\n",
      "print label_word_fd['neg'].most_common(nn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'the', 102595)]\n",
        "[(u'the', 40929)]\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from threading import Thread\n",
      "class ModelWorker(Thread):\n",
      "   def __init__(self, queue, acc):\n",
      "       Thread.__init__(self)\n",
      "       self.queue = queue\n",
      "       self.ls=acc\n",
      "\n",
      "   def run(self):\n",
      "       while len(self.queue)>0:\n",
      "           # Get the work from the queue and expand the tuple       \n",
      "           minNgramFreq = self.queue.pop()\n",
      "           print 'done'\n",
      "           self.ls+=[evaluate_classifier(data,best_ngram_word_feats)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "worker = ModelWorker([2,3,5,10,50,100,500,1000],[])\n",
      "       # Setting daemon to True will let the main thread exit even though the workers are blocking\n",
      "worker.daemon = True\n",
      "worker.start()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done\n"
       ]
      }
     ],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "worker.ls"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 203,
       "text": [
        "[]"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# a_documents = reviews.words[reviews.review_rating.astype(float)>4.5]\n",
      "# b_documents = reviews.words[reviews.review_rating.astype(float)==4]\n",
      "# c_documents = reviews.words[reviews.review_rating.astype(float)==3]\n",
      "# d_documents = reviews.words[reviews.review_rating.astype(float)<2.5]\n",
      "# data = {\n",
      "#         '5':a_documents,\n",
      "#         '4':b_documents,\n",
      "#         '3':c_documents,\n",
      "#         '2':d_documents\n",
      "#         }\n",
      "\n",
      "\n",
      "#only take the top nBest important features of all into account\n",
      "nBestFeatures=55\n",
      "#ignore words with <minFreq\n",
      "minFreq=150\n",
      "#top n ngrams for EACH review\n",
      "nBestNGrams=50\n",
      "#ignore ngrams with <#ignore words with <minFreq\n",
      "minNgramFreq=150\n",
      "\n",
      "\n",
      "print 'evaluating single word features'\n",
      "#evaluate_classifier(data,word_feats)\n",
      "\n",
      "\n",
      " \n",
      "print 'evaluating best word features'\n",
      "#evaluate_classifier(data,best_word_feats)\n",
      "\n",
      "\n",
      "print 'evaluating most informative word features'\n",
      "#evaluate_classifier(data,informative_word_feats)\n",
      "\n",
      " \n",
      "print 'evaluating best words + bigram chi_sq word features'\n",
      "evaluate_classifier(data,best_ngram_word_feats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "evaluating single word features\n",
        "evaluating best word features\n",
        "evaluating most informative word features\n",
        "evaluating best words + bigram chi_sq word features\n",
        "negsample = 5372"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "possample = 16303\n",
        "training size:  16256, testing size: 5419\n",
        "accuracy:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.816940394907\n",
        "pos precision: 0.887048192771\n",
        "pos recall: 0.867026496565\n",
        "neg precision: 0.622299651568\n",
        "neg recall: 0.664929262844\n",
        "Most Informative Features\n",
        "(u'will', u'definitely', u'be') = True              pos : neg    =     34.8 : 1.0\n",
        "                   worst = True              neg : pos    =     18.5 : 1.0\n",
        "     (u\"can't\", u'wait') = True              pos : neg    =     15.5 : 1.0\n",
        "        (u'3', u'stars') = True              neg : pos    =     15.2 : 1.0\n",
        "(u'definitely', u'be', u'back') = True              pos : neg    =     13.5 : 1.0\n",
        "     (u'hands', u'down') = True              pos : neg    =     12.0 : 1.0\n",
        "   (u'my', u'favorites') = True              pos : neg    =     11.3 : 1.0\n",
        "                    rude = True              neg : pos    =     10.6 : 1.0\n",
        "       (u'melts', u'in') = True              pos : neg    =     10.4 : 1.0\n",
        "                horrible = True              neg : pos    =     10.1 : 1.0\n",
        "                terrible = True              neg : pos    =      9.6 : 1.0\n",
        "                mediocre = True              neg : pos    =      9.5 : 1.0\n",
        " (u'be', u'sure', u'to') = True              pos : neg    =      8.8 : 1.0\n",
        "           disappointing = True              neg : pos    =      8.1 : 1.0\n",
        "(u\"can't\", u'go', u'wrong') = True              pos : neg    =      7.4 : 1.0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 141,
       "text": [
        "0.8169403949068094"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "positive_docs=[' '.join(words) for words in reviews.words[reviews.review_rating.astype(float)>3.5]]\n",
      "negative_docs=[' '.join(words) for words in reviews.words[reviews.review_rating.astype(float)<3.5]]\n",
      "y=['pos']*len(positive_docs)+['neg']*len(negative_docs)\n",
      "train_x = positive_docs[:len(positive_docs)*3/4]+negative_docs[:len(negative_docs)*3/4]\n",
      "train_y = ['pos']*(len(positive_docs)*3/4)+['neg']*(len(negative_docs)*3/4)\n",
      "test_x = positive_docs[len(positive_docs)*3/4:]+negative_docs[len(negative_docs)*3/4:]\n",
      "test_y = ['pos']*(len(positive_docs)-len(positive_docs)*3/4)+['neg']*(len(negative_docs)-len(negative_docs)*3/4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer()\n",
      "X_train_features = vectorizer.fit_transform(train_x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "NBclf = MultinomialNB()\n",
      "model = NBclf.fit_transform(positive_docs+negative_docs,y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'MultinomialNB' object has no attribute 'fit_transform'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-89-c3563a1dc48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNBclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNBclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_docs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnegative_docs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: 'MultinomialNB' object has no attribute 'fit_transform'"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(vectorizer.get_feature_names())\n",
      "X_train_features.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "18685\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 342,
       "text": [
        "(8318, 18685)"
       ]
      }
     ],
     "prompt_number": 342
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_selection import SelectKBest, chi2\n",
      "ch2 = SelectKBest(chi2, k = 5000)\n",
      "K_best = ch2.fit_transform(X_train_features, train_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 346
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.asarray(vectorizer.get_feature_names())[ch2.get_support()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'00' u'007' u'008' ..., u'zhajiangmian' u'zuma' u'zzzzzzzzz']\n"
       ]
      }
     ],
     "prompt_number": 351
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in reviews.index[reviews.review_rating.astype(float)==5]:\n",
      "    if 'addictingto' in reviews.words[i]:\n",
      "        print reviews.loc[i].review\n",
      "        print reviews.loc[i].words\n",
      "        print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 314
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blf= best_word_classifier(data) \n",
      "mif = show_most_informative_features(blf, word_fd,50)\n",
      "#word_fd['']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n"
       ]
      }
     ],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = [(x[0],word_fd[x[0]],label_word_fd['pos'][x[0]]) for x in mif[1:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 226,
       "text": [
        "[(u'worst', 286, 39),\n",
        " (u'rude', 319, 63),\n",
        " (u'horrible', 249, 54),\n",
        " (u'terrible', 308, 70),\n",
        " (u'mediocre', 431, 87),\n",
        " (u'disappointing', 387, 101),\n",
        " (u'meh', 212, 52),\n",
        " (u'overpriced', 324, 92),\n",
        " (u'understand', 420, 156),\n",
        " (u'manager', 476, 133),\n",
        " (u'bland', 460, 171),\n",
        " (u'average', 745, 286),\n",
        " (u'ok', 1437, 581),\n",
        " (u'perfect', 1448, 1336),\n",
        " (u'okay', 933, 388),\n",
        " (u'asked', 1332, 583),\n",
        " (u'amazing', 3360, 3052),\n",
        " (u'nothing', 1577, 768),\n",
        " (u'seattle', 3610, 3111),\n",
        " (u'delicious', 4632, 4143),\n",
        " (u'told', 1263, 573),\n",
        " (u'maybe', 1361, 700),\n",
        " (u'decent', 1228, 624),\n",
        " (u'minutes', 2002, 961),\n",
        " (u'excellent', 2063, 1833),\n",
        " (u'said', 1896, 953),\n",
        " (u'awesome', 1753, 1575),\n",
        " (u'favorite', 3086, 2743),\n",
        " (u'love', 4411, 3896),\n",
        " (u'her', 1422, 697),\n",
        " (u\"wasn't\", 2375, 1257),\n",
        " (u'waitress', 1627, 860),\n",
        " (u\"didn't\", 3714, 1992),\n",
        " (u'bad', 2788, 1564),\n",
        " (u'she', 3006, 1431),\n",
        " (u'always', 3784, 3326),\n",
        " (u'recommend', 2442, 2110),\n",
        " (u'better', 3248, 1901),\n",
        " (u'best', 5596, 4817),\n",
        " (u'after', 3777, 2238),\n",
        " (u'was', 48864, 32076),\n",
        " (u'great', 10618, 9046),\n",
        " (u'not', 16789, 9885),\n",
        " (u'not', 16789, 9885),\n",
        " (u'definitely', 4497, 3753),\n",
        " (u'happy', 6178, 5086),\n",
        " (u'us', 5661, 3334),\n",
        " (u'but', 25401, 16990),\n",
        " (u'fresh', 6003, 4865),\n",
        " (u'were', 14049, 9019)]"
       ]
      }
     ],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}